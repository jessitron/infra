NAME: my-opentelemetry-collector
LAST DEPLOYED: Thu Sep  7 10:24:58 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
HOOKS:
MANIFEST:
---
# Source: opentelemetry-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-opentelemetry-collector
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-collector-0.67.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: my-opentelemetry-collector
    app.kubernetes.io/version: "0.84.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: opentelemetry-collector/templates/configmap-agent.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-opentelemetry-collector-agent
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-collector-0.67.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: my-opentelemetry-collector
    app.kubernetes.io/version: "0.84.0"
    app.kubernetes.io/managed-by: Helm
data:
  relay: |
    exporters:
      logging: {}
      otlp:
        endpoint: api.honeycomb.io:443
        headers:
          x-honeycomb-team: ${HONEYCOMB_API_KEY}
      otlp/logging:
        endpoint: api.honeycomb.io:443
        headers:
          x-honeycomb-dataset: service-logs
          x-honeycomb-team: ${HONEYCOMB_API_KEY}
      otlp/metrics:
        endpoint: api.honeycomb.io:443
        headers:
          x-honeycomb-dataset: service-metrics
          x-honeycomb-team: ${HONEYCOMB_API_KEY}
    extensions:
      health_check: {}
      memory_ballast:
        size_in_percentage: 40
    processors:
      batch: {}
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
    service:
      extensions:
      - health_check
      - memory_ballast
      pipelines:
        logs:
          exporters:
          - otlp/logging
          processors:
          - k8sattributes
          - memory_limiter
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - otlp/metrics
          processors:
          - k8sattributes
          - memory_limiter
          - batch
          receivers:
          - otlp
          - prometheus
        traces:
          exporters:
          - otlp
          processors:
          - k8sattributes
          - memory_limiter
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
---
# Source: opentelemetry-collector/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.67.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: my-opentelemetry-collector
    app.kubernetes.io/version: "0.84.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: opentelemetry-collector/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-opentelemetry-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.67.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: my-opentelemetry-collector
    app.kubernetes.io/version: "0.84.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-opentelemetry-collector
subjects:
- kind: ServiceAccount
  name: my-opentelemetry-collector
  namespace: default
---
# Source: opentelemetry-collector/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-opentelemetry-collector-agent
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-collector-0.67.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: my-opentelemetry-collector
    app.kubernetes.io/version: "0.84.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/instance: my-opentelemetry-collector
      component: agent-collector
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: d0fc20e565949a3e368cd8a2650af9688fe4b0b5219156e8b5a566c9036ef4dd
        
      labels:
        app.kubernetes.io/name: opentelemetry-collector
        app.kubernetes.io/instance: my-opentelemetry-collector
        component: agent-collector
        
    spec:
      
      serviceAccountName: my-opentelemetry-collector
      securityContext:
        {}
      containers:
        - name: opentelemetry-collector
          command:
            - /otelcol-contrib
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "otel/opentelemetry-collector-contrib:0.84.0"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
              hostPort: 6831
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
              hostPort: 14250
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
              hostPort: 14268
            - name: otlp
              containerPort: 4317
              protocol: TCP
              hostPort: 4317
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
              hostPort: 4318
            - name: zipkin
              containerPort: 9411
              protocol: TCP
              hostPort: 9411
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: HONEYCOMB_API_KEY
              value: FecmbNqfOaYqsELVH54OrD
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              cpu: 1
              memory: 1Gi
          volumeMounts:
            - mountPath: /conf
              name: opentelemetry-collector-configmap
      volumes:
        - name: opentelemetry-collector-configmap
          configMap:
            name: my-opentelemetry-collector-agent
            items:
              - key: relay
                path: relay.yaml
      hostNetwork: false

NOTES:
[INFO] as of chart version 0.47.0 the default collector configuration has been updated to use pod IP instead of 0.0.0.0 for its endpoints. See https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/UPGRADING.md#0460-to-0470 for details.
